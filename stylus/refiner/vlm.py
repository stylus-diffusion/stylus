from concurrent.futures import ThreadPoolExecutor, as_completed
import io
import os
import pickle
import time
from typing import List, Optional, Tuple

import tqdm
import urllib.request
from PIL import Image as PIL_Image

from google.cloud.aiplatform_v1beta1.types.content import SafetySetting
from stylus.refiner.fetch_adapter_metadata import AdapterInfo, fetch_adapter_metadata
from vertexai.generative_models import GenerativeModel, HarmBlockThreshold, HarmCategory
from vertexai.generative_models import Image as VertexImage
import vertexai

from stylus.refiner.fetch_adapter_metadata import ADAPTERS_FILE, AdapterInfo, fetch_adapter_metadata

from openai import OpenAI
import base64
import argparse


# Constants
CACHE_RES_DIR = "cache/ultra_model"
HARM_CATEGORIES = [
    HarmCategory.HARM_CATEGORY_UNSPECIFIED,
    HarmCategory.HARM_CATEGORY_HARASSMENT,
    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
    HarmCategory.HARM_CATEGORY_HATE_SPEECH,
]
VLM_PROMPT = """
Your goal is to improve the description of a model adapter for Stable Diffusion, with images and description pulled from popular model repositories from Huggingface or Civitai. Above, we have provided:

1) The model card of the adapter, provided by the original author. This includes title, tags, trigger words, and description.
- The model card may be incorrect, misleading, or incomplete, so use the images as a source of truth.
- The model card may specify the weight of the model adapter, or the recommended range. Find the recommended weight of the LoRA. Default weight is 0.8. If a range of weights is specified, give the average of the range.

2) Images, which may be NSFW, are generated by the model adapter (i.e. LoRA) with the corresponding user-provided prompts.
- If no prompt is found, "None" is specified.
- Some prompts may specify the adapter weight, in the form of <lora:[NAME]:[WEIGHT]>, and the associated trigger words. You will need to infer the adapter NAME and WEIGHT from the prompt. If a weight is found, override the model card weight recommendation.
- Isolate the effects of the model adapter on the image apart from other text in the prompt.

Your goal is to provide a better, more concise, description of the model adapter and its impact on the image. You should implicitly categorize the model adapters into only one of the following topics: Concept, Style, Pose, Action, Celebrity/character, Clothing, Background, Building, Vehicle, Animal, Action. Do not identify an adapter with an associated topic that is vague, generic, or uninteresting.

First, describe the main topic that the adapter introduces and how this adapter modifies the commonalities between all provided images. Your requirements are:
- Do not go over 1000 tokens.
- Do not hallucinate and repeat text. Output only english words and sentences.
- Do not describe any training details and dataset related to the LoRA.
- Provide additional context from your pre-trained knowledge if there is not enough information. Again, do not hallucinate.

Second, recommend an optimal weight for the adapter as a float. Do not specify a range, only give one value.

The output format should be:

- [DESCRIPTION]
- [RECOMMENDED WEIGHT]

Give your answer in the output format above with two bullet points only.

We have provided example responses below that follow the output format. Do not copy from these responses.

Example Response 1:

- This LoRA is the for celebrity, Sean Connery, an actor from the James Bond franchise. Sean Connery is depicted as a white man with a classic comb-over hairstyle, well-groomed eyebrows, chiseled facial strcuture, brown eyes, and a clean-shaven face. Between 1962 and 1967, Connery played 007 in Dr. No, From Russia with Love, Goldfinger, Thunderball, and You Only Live Twice, the first five Bond films produced by Eon Productions. James Bond is a fictional Senior Operational Officer of an covert Black Ops unit within the Secret Intelligence Service, MI6. As an agent of MI6, Bond holds the cryptonym "007".
- 0.

Example Response 2:

- This LoRA steers the image generation towards the classical Chinese ink painting style, replicating the nuanced brushwork and ink wash techniques seen in the works of historical Chinese painting masters, such as Anhui Wu Warehouse, Xinghua Banqiao, and Bada Shanren. This effect creates a distinct monochromatic palette, with varied ink intensities providing depth and texture. The adapter particularly affects the stylistic rendering of figures and botanical elements. The figures and flora depicted in the images showcase characteristics of "Xieyi" or freehand style, focusing on capturing the spirit rather than the exact likeness. This adapter also seems to incorporate elements of "Gongbi," a meticulous brush technique. The overall aesthetic resonates with the Ming and Qing dynasties' art and gives prominence to flowing garments, refined postures, and the subtle gradations of ink, transforming modern subjects into classic works with a historical flair.
- 0.85

Example Response 3:

- This LoRA is designed to generate highly detailed futuristic cityscapes. It generates images of a cityscape with a futuristic aesthetic, featuring towering skyscrapers, sleek and modern architecture, vibrant and neon-lit atmosphere, sleek and curvilinear buildings, and intricate transport systems featuring autonomous vehicles and drones dotting the sky.. The cityscape is characterized by its advanced technology, with holographic billboards, flying vehicles, and bustling crowds. The adapter demonstrates versatility by applying different atmospheric conditions such as bright daylight, reflective wet surfaces during rain, and the warm hues of sunset, all the while ensuring that the cities appear technologically advanced. The overall atmosphere is one of progress and innovation, with a focus on the potential of future urban environments. The primary focus is on the background and building aspects of the images, where it adds elements that resonate with a vision of progress, sustainability, and high-tech urban life.
- 0.5

Again, give your answer with the output format above. Recommended weight should only be a float, do not attach additional description beyond that.
"""


def check_cache(adapter_id: str) -> bool:
    """Check if VLM output is already cached."""
    cur_dir = os.path.dirname(os.path.abspath(__file__))
    parent_dir = os.path.dirname(cur_dir)
    file_path = os.path.join(parent_dir, f'{CACHE_RES_DIR}/{adapter_id}.txt')
    os.makedirs(os.path.dirname(file_path), exist_ok=True)
    return os.path.exists(file_path)


def write_cache(adapter_id: str, data: str):
    """Write data to cache for VLM description."""
    cur_dir = os.path.dirname(os.path.abspath(__file__))
    parent_dir = os.path.dirname(cur_dir)
    file_path = os.path.join(parent_dir, f'{CACHE_RES_DIR}/{adapter_id}.txt')
    os.makedirs(os.path.dirname(file_path), exist_ok=True)
    with open(file_path, 'w') as file:
        file.write(data)


def parse_model_output(input_string: str) -> Tuple[str, float]:
    """Parse the model output to extract description and weight."""
    parts = input_string.split('- ')
    description = ''.join(parts[1:-1]).strip()
    weight = float(parts[-1].strip())
    return description, weight


def load_image_from_url_gemini(image_url: str) -> Optional[VertexImage]:
    """Load an image from a URL and return it as a VertexImage object."""
    try:
        req = urllib.request.Request(image_url,
                                     headers={'User-Agent': 'Mozilla/5.0'})
        with urllib.request.urlopen(req, timeout=10) as response:
            # Check if the content type is an image
            if 'image' not in response.headers['Content-Type']:
                return None
            image_bytes = response.read()
            # Try to open the image with PIL to verify it's valid
            try:
                pil_image = PIL_Image.open(io.BytesIO(image_bytes))
                pil_image.verify()  # Verify that it is an image
            except PIL_Image.UnidentifiedImageError:
                return None
            return VertexImage.from_bytes(image_bytes)
    except Exception as e:
        print(f"Failed to download or validate image from {image_url}: {e}")
        return None


def load_image_from_url_openai(image_url: str) -> str:
    """Load an image from a URL and return it as a VertexImage object."""
    try:
        req = urllib.request.Request(image_url,
                                     headers={'User-Agent': 'Mozilla/5.0'})
        with urllib.request.urlopen(req, timeout=10) as response:
            # Check if the content type is an image
            if 'image' not in response.headers['Content-Type']:
                return None
            image_bytes = response.read()
            
            if len(image_bytes) >= 2 * 1024 * 1024:
                print(f"Image size is too large: {len(image_bytes)} bytes")
                return None
            # Try to open the image with PIL to verify it's valid
            try:
                pil_image = PIL_Image.open(io.BytesIO(image_bytes))
                pil_image.verify()  # Verify that it is an image
            except PIL_Image.UnidentifiedImageError:
                return None
            return base64.b64encode(image_bytes).decode("utf-8")
    except Exception as e:
        print(f"Failed to download or validate image from {image_url}: {e}")


def poll_model_with_backoff_gemini(prompt_list: List[str],
                            model,
                            max_attempts=1e9,
                            safety_settings=None):
    """Attempt to fetch model response with exponential backoff."""
    backoff_time = 1
    counter = 0
    while True:
        try:
            model_response = model.generate_content(
                prompt_list, safety_settings=safety_settings)
            return model_response
        except Exception as e:
            print(f"Error during model generation: {e}")
            import traceback
            print(traceback.print_exc())
            counter += 1
            if counter >= max_attempts:
                return None
            time.sleep(backoff_time)
            backoff_time = min(backoff_time * 2, 64)


def poll_model_with_backoff_openai(prompt_list: List[str],
                            model,
                            client,
                            max_attempts=1e9):
    """Attempt to fetch model response with exponential backoff."""
    backoff_time = 1
    counter = 0
    while True:
        try:
            model_response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": prompt_list}
                ],
                temperature=0.0,
            )
            return model_response
        except Exception as e:
            print(f"Error during model generation: {e}")
            import traceback
            print(traceback.print_exc())
            counter += 1
            if counter >= max_attempts:
                return None
            time.sleep(backoff_time)
            backoff_time = min(backoff_time * 2, 64)



# NOTE: Stylus Docs uses `gemini-1.0-ultra-vision`, an outdated model.
# Since Ultra-Vision is not available anymore, we use Gemini 1.5 instead.
class GeminiVLM:
    """A class representing the Gemini VLM."""

    def __init__(self, model='gemini-1.5-pro-001'):
        vertexai.init()
        self.model = GenerativeModel(model)
        self.safety_settings = [
            SafetySetting(category=h, threshold=HarmBlockThreshold.BLOCK_NONE)
            for h in HARM_CATEGORIES
        ]
        self.retry_attempts = 3

    def assemble_vlm_prompt(self, adapter: AdapterInfo, max_images=10):
        prompt_list = [
            f"Title: {adapter.title}; "
            f"Tags: {adapter.tags}; "
            f"Trigger Words: {adapter.trigger_words}; "
            f"Description: {adapter.description}"
        ]

        # Ensuring the lengths of image related lists are consistent
        if not (len(adapter.image_urls) == len(adapter.image_prompts) == len(
                adapter.image_negative_prompts)):
            raise ValueError(
                "Image URLs, prompts, and negative prompts must be the same length."
            )

        # Processing images and their corresponding prompts
        image_data = zip(adapter.image_urls, adapter.image_prompts)
        counter = 0
        for image_url, image_prompt in image_data:
            if counter >= max_images:
                break
            image = load_image_from_url_gemini(image_url)
            if not image:
                continue
            # Add the successfully loaded image and its prompt to the list
            # Exclude negative prompts as this can confused the VLM.
            prompt_list.extend(
                [image, 'Prompt: ' + image_prompt.replace("\n", "")])
            counter += 1

        # Append the static VLM prompt at the end
        prompt_list.append(VLM_PROMPT)
        return prompt_list

    def generate(self, adapter: AdapterInfo, max_images=10):
        """
        Generates the VLM description for the given adapter and caches the result.

        Args:
            adapter (AdapterInfo): The adapter information containing the metadata for VLM generation.
            max_images (int): The maximum number of images to consider for generating the description.

        Returns:
            None: Results are printed and cached directly within the function.
        """
        adapter_id = adapter.adapter_id
        if check_cache(adapter_id):
            # If the VLM output is already cached, return nothing.
            return

        prompt_list = self.assemble_vlm_prompt(adapter, max_images)
        counter = 0
        while True:
            if counter > self.retry_attempts:
                print("=" * 9)
                print(f'{adapter.title} ;; {adapter.alias}')
                print("Failed to compute VLM description.")
                print("=" * 9)
                write_cache(adapter_id, 'Failed')
                break

            try:
                model_response = poll_model_with_backoff_gemini(
                    prompt_list,
                    self.model,
                    safety_settings=self.safety_settings)
                # If model errored (such as safety filters triggered), this will create an error (as text does not exist).
                model_response.text
                description, weight = parse_model_output(model_response.text)
            except (IndexError, Exception):
                counter += 1
                continue

            if getattr(model_response, '_raw_response', None) and \
            model_response._raw_response.usage_metadata.candidates_token_count > 1000:
                # Ultra model can output repeating text until end of output decoding length.
                counter += 1
                continue

            print("=" * 9)
            print(f'{adapter.title} ;; {adapter.alias}')
            print(f'Description: {description} ;; Weight: {weight}')
            print("=" * 9)
            write_cache(adapter_id, model_response.text)
            break


class OpenAIVLM:
    """A class representing the OpenAI VLM."""

    def __init__(self, model='gpt-4o'):
        self.model = model
        self.client = OpenAI()
        self.retry_attempts = 2

    def assemble_vlm_prompt(self, adapter: AdapterInfo, max_images=7):
        # Put the static VLM prompt at the first
        prompt_list =[]
        prompt_list += [{
            "type": "text",
            "text":
            f"Title: {adapter.title}; "
            f"Tags: {adapter.tags}; "
            f"Trigger Words: {adapter.trigger_words}; "
            f"Description: {adapter.description}"
        }]

        # Ensuring the lengths of image related lists are consistent
        if not (len(adapter.image_urls) == len(adapter.image_prompts) == len(
                adapter.image_negative_prompts)):
            raise ValueError(
                "Image URLs, prompts, and negative prompts must be the same length."
            )

        # Processing images and their corresponding prompts
        image_data = zip(adapter.image_urls, adapter.image_prompts)
        counter = 0
        for image_url, image_prompt in image_data:
            if counter >= max_images:
                break

            _, image_ext = os.path.splitext(image_url)
            image_ext = image_ext[1:].strip()
            image = load_image_from_url_openai(image_url)

            # check valid image format for openai api
            if image_ext not in ['png', 'jpg', 'jpeg', 'gif', 'webp']:
                continue
            if image_ext == 'jpg':
                image_ext == 'jpeg'
            if not image:
                continue

            # Add the successfully loaded image and its prompt to the list
            # Exclude negative prompts as this can confused the VLM.
            prompt_list += [{"type": "image_url", "image_url": {"url": f"data:image/{image_ext};base64,{image}"}}]
            prompt_list += [{"type": "text", "text": 'Prompt: ' + image_prompt.replace("\n", "")}]
            counter += 1
        prompt_list.append({"type": "text", "text": VLM_PROMPT})
        return prompt_list

    def generate(self, adapter: AdapterInfo, max_images=10):
        """
        Generates the VLM description for the given adapter and caches the result.

        Args:
            adapter (AdapterInfo): The adapter information containing the metadata for VLM generation.
            max_images (int): The maximum number of images to consider for generating the description.

        Returns:
            None: Results are printed and cached directly within the function.
        """
        adapter_id = adapter.adapter_id
        if check_cache(adapter_id):
            # If the VLM output is already cached, return nothing.
            print("Cached")
            return
        prompt_list = self.assemble_vlm_prompt(adapter, max_images)
        counter = 0
        while True:
            if counter > self.retry_attempts:
                print("=" * 9)
                print(f'{adapter.title} ;; {adapter.alias}')
                print("Failed to compute VLM description.")
                print("=" * 9)
                write_cache(adapter_id, 'Failed')
                break

            try:
                model_response = poll_model_with_backoff_openai(
                    prompt_list,
                    self.model,
                    self.client)
                model_response_text = model_response.choices[0].message.content
                # If model errored (such as safety filters triggered), this will create an error (as text does not exist).
                description, weight = parse_model_output(model_response_text)
            except (IndexError, Exception):
                counter += 1
                continue

            print("=" * 9)
            print(f'{adapter.title} ;; {adapter.alias}')
            print(f'Description: {description} ;; Weight: {weight}')
            print("=" * 9)
            write_cache(adapter_id, model_response_text)
            break


SUPPORTED_VLM_MODELS = {
    'gemini': GeminiVLM,
    'openai': OpenAIVLM,
}


def compute_vlm_description(adapters: List[AdapterInfo],
                            describe_model='gemini'):
    """Compute VLM descriptions for a list of adapters in parallel."""

    def _process_vlm(adapter, describe_model='gemini'):
        """Function to process a single adapter."""
        model = SUPPORTED_VLM_MODELS[describe_model]()
        txt_response = model.generate(adapter)
        return txt_response  # Assuming you want to collect these

    # This throughput for VLM description is bounded by GCP quota for Gemini models.
    # See: https://cloud.google.com/vertex-ai/generative-ai/docs/quotas
    with ThreadPoolExecutor(max_workers=64) as executor:
        # Schedule the processing of each adapter to be executed in parallel
        future_to_adapter = {
            executor.submit(_process_vlm, adapter, describe_model): adapter
            for adapter in adapters
        }
        with tqdm.tqdm(total=len(adapters)) as pbar:
            for future in as_completed(future_to_adapter):
                adapter = future_to_adapter[future]
                try:
                    # Retrieve result from future
                    future.result()
                except Exception as exc:
                    print(f'{adapter.title} generated an exception: {exc}')
                pbar.update(1)


def move_vlm_description(adapters: List[AdapterInfo]):
    """Moved cached VLM descriptions to AdapterInfo.llm_description."""

    def _add_vlm_description(adapter: AdapterInfo):
        """Fetch and assign the cached LLM description to a single adapter."""
        adapter_id = adapter.adapter_id
        file_path = os.path.join(os.path.abspath(os.path.dirname(__file__)),
                                 '..', CACHE_RES_DIR, f'{adapter_id}.txt')
        if not os.path.exists(file_path):
            return adapter
        with open(file_path, 'r') as file:
            txt_response = file.read()
        if txt_response == 'Failed':
            return adapter
        description, weight = parse_model_output(txt_response)
        adapter.llm_description = description
        adapter.weight = weight
        return adapter

    adapter_list = []
    with ThreadPoolExecutor(max_workers=128) as executor:
        future_to_adapter = {
            executor.submit(_add_vlm_description, adapter): adapter
            for adapter in adapters
        }
        with tqdm.tqdm(total=len(adapters)) as pbar:
            for future in as_completed(future_to_adapter):
                result_adapter = future.result()
                if result_adapter:
                    adapter_list.append(result_adapter)
                pbar.update(1)

    # Sorting the adapter list by adapter_id and saving it to a pickle file
    adapter_list.sort(key=lambda x: x.adapter_id)
    save_path = os.path.join(os.path.abspath(os.path.dirname(__file__)), '..',
                             ADAPTERS_FILE)
    with open(save_path, 'wb') as file:
        pickle.dump(adapter_list, file)


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Run VLM Model.')
    parser.add_argument('--vlm', type=str, help='VLM Model Name (gemini or openai)', default='gemini')
    args = parser.parse_args()

    adapter_list = fetch_adapter_metadata(base_model='SD 1.5',
                                          adapter_type='LORA')

    # Depending on user's quota, this might take days or weeks to complete.
    compute_vlm_description(adapter_list, describe_model=args.vlm)

    # Take the cached VLM descriptions and save them into AdapterInfo.llm_description.
    # This completes StylusDocs.
    move_vlm_description(adapter_list)
